{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN) Implementation\n",
    "\n",
    "This notebook demonstrates a complete implementation of an Artificial Neural Network for binary classification.\n",
    "\n",
    "## Dataset\n",
    "We'll use the Churn Modeling dataset to predict customer churn.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Data Preprocessing\n",
    "2. Feature Scaling\n",
    "3. Building ANN Architecture\n",
    "4. Training the Model\n",
    "5. Evaluating Performance\n",
    "6. Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll create a synthetic dataset\n",
    "# You can replace this with your own dataset\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, \n",
    "                          n_redundant=5, random_state=42)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'Feature_{i+1}' for i in range(20)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Target'] = y\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Target', data=df)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Target Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = df.corr()\n",
    "sns.heatmap(correlation, cmap='coolwarm', center=0, linewidths=1, annot=False)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = df.drop('Target', axis=1).values\n",
    "y = df['Target'].values\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature Scaling Complete\")\n",
    "print(f\"Mean of first feature in training: {X_train[:, 0].mean():.4f}\")\n",
    "print(f\"Std of first feature in training: {X_train[:, 0].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build ANN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=100, \n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict for a single sample\n",
    "sample = X_test[0].reshape(1, -1)\n",
    "prediction_prob = model.predict(sample)\n",
    "prediction_class = (prediction_prob > 0.5).astype(int)[0][0]\n",
    "\n",
    "print(f\"Prediction Probability: {prediction_prob[0][0]:.4f}\")\n",
    "print(f\"Predicted Class: {prediction_class}\")\n",
    "print(f\"Actual Class: {y_test[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('ann_model.h5')\n",
    "print(\"Model saved as 'ann_model.h5'\")\n",
    "\n",
    "# To load the model later:\n",
    "# from tensorflow.keras.models import load_model\n",
    "# loaded_model = load_model('ann_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **ANN Architecture**: Multi-layer perceptron with dense layers\n",
    "2. **Activation Functions**: ReLU for hidden layers, Sigmoid for output\n",
    "3. **Regularization**: Dropout layers to prevent overfitting\n",
    "4. **Optimization**: Adam optimizer with binary crossentropy loss\n",
    "5. **Early Stopping**: Prevents overfitting by monitoring validation loss\n",
    "\n",
    "### When to Use ANN:\n",
    "- Binary or multi-class classification\n",
    "- Regression problems\n",
    "- Pattern recognition in structured/tabular data\n",
    "- When you have sufficient training data\n",
    "\n",
    "### Advantages:\n",
    "- Can learn complex non-linear relationships\n",
    "- Flexible architecture\n",
    "- Good performance on various tasks\n",
    "\n",
    "### Limitations:\n",
    "- Requires significant training data\n",
    "- Computationally expensive\n",
    "- Can overfit without proper regularization\n",
    "- Acts as a \"black box\" (less interpretable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
