{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Implementation\n",
    "\n",
    "This notebook demonstrates LSTM networks which solve the vanishing gradient problem of traditional RNNs.\n",
    "\n",
    "## Dataset\n",
    "IMDB movie reviews for sentiment analysis.\n",
    "\n",
    "## Topics Covered:\n",
    "1. LSTM Architecture and Gates\n",
    "2. Long-term Dependency Learning\n",
    "3. Memory Cell Mechanism\n",
    "4. Comparison with SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, BatchNormalization\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution: Positive={np.sum(y_train)}, Negative={len(y_train)-np.sum(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(vocab_size, 128, input_length=max_length),\n",
    "    \n",
    "    # LSTM layers with dropout\n",
    "    LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "checkpoint = ModelCheckpoint('best_lstm_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "callbacks = [early_stop, reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=15,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], marker='o', label='Training Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], marker='s', label='Validation Accuracy')\n",
    "axes[0].set_title('LSTM Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], marker='o', label='Training Loss')\n",
    "axes[1].plot(history.history['val_loss'], marker='s', label='Validation Loss')\n",
    "axes[1].set_title('LSTM Model Loss', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nPredictions made: {len(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix - LSTM', fontsize=14)\n",
    "plt.ylabel('Actual Sentiment')\n",
    "plt.xlabel('Predicted Sentiment')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - LSTM Sentiment Analysis')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i >= 3])\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"Sample Predictions:\\n\" + \"=\"*80)\n",
    "for i in range(5):\n",
    "    review = decode_review(X_test[i])\n",
    "    actual = 'Positive' if y_test[i] == 1 else 'Negative'\n",
    "    predicted = 'Positive' if y_pred[i] == 1 else 'Negative'\n",
    "    confidence = y_pred_prob[i][0]\n",
    "    \n",
    "    print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"Text: {review[:150]}...\")\n",
    "    print(f\"Actual: {actual} | Predicted: {predicted} | Confidence: {confidence:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "model.save('lstm_sentiment_model.h5')\n",
    "print(\"Model saved as 'lstm_sentiment_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### LSTM Architecture:\n",
    "LSTMs use a gating mechanism to control information flow:\n",
    "\n",
    "1. **Forget Gate**: Decides what to discard from cell state\n",
    "2. **Input Gate**: Decides what new information to store\n",
    "3. **Cell State**: Long-term memory\n",
    "4. **Output Gate**: Decides what to output\n",
    "\n",
    "### Key Advantages:\n",
    "- **Long-term Dependencies**: Remembers information for extended periods\n",
    "- **Vanishing Gradient Solution**: Gates prevent gradient vanishing\n",
    "- **Better Accuracy**: Superior performance on sequence tasks\n",
    "- **Flexible**: Works with variable-length sequences\n",
    "\n",
    "### When to Use LSTM:\n",
    "- Long sequences (>100 time steps)\n",
    "- Complex dependencies\n",
    "- Text generation\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Time series forecasting\n",
    "\n",
    "### LSTM vs SimpleRNN:\n",
    "- **LSTM**: Better for long sequences, more parameters, slower\n",
    "- **SimpleRNN**: Faster, simpler, but struggles with long sequences\n",
    "- LSTM typically achieves 3-5% better accuracy\n",
    "- LSTM is the default choice for most sequence tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
