{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🫀 Heart Disease Prediction using Logistic Regression\n",
    "\n",
    "## 📊 Project Overview\n",
    "\n",
    "This notebook demonstrates **Logistic Regression** for binary classification - predicting whether a patient has heart disease or not.\n",
    "\n",
    "### 🎯 Learning Objectives:\n",
    "- Understand Logistic Regression for classification\n",
    "- Compare with Linear Regression\n",
    "- Implement complete ML pipeline\n",
    "- Evaluate classification models\n",
    "- Feature importance analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Linear vs Logistic Regression\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Aspect | Linear Regression | Logistic Regression |\n",
    "|--------|------------------|--------------------|\n",
    "| **Purpose** | Predict continuous values | Predict probabilities/classes |\n",
    "| **Output** | Real number (-∞ to +∞) | Probability (0 to 1) |\n",
    "| **Use Case** | Car prices, house prices | Disease prediction, spam detection |\n",
    "| **Function** | y = mx + b | y = 1/(1 + e^-(mx+b)) |\n",
    "| **Loss Function** | Mean Squared Error (MSE) | Log Loss (Cross-Entropy) |\n",
    "| **Evaluation** | R², RMSE, MAE | Accuracy, Precision, Recall, F1, ROC-AUC |\n",
    "| **Algorithm** | Ordinary Least Squares | Maximum Likelihood Estimation |\n",
    "| **Decision Boundary** | Not applicable | Linear boundary in feature space |\n",
    "| **Assumptions** | Linear relationship | Log-odds are linear |\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "**Use Linear Regression when:**\n",
    "- ✅ Predicting continuous numerical values\n",
    "- ✅ Output can be any real number\n",
    "- ✅ Examples: prices, temperatures, sales\n",
    "\n",
    "**Use Logistic Regression when:**\n",
    "- ✅ Predicting categories or probabilities\n",
    "- ✅ Binary classification (Yes/No, True/False)\n",
    "- ✅ Examples: spam/not spam, disease/healthy\n",
    "\n",
    "### Visual Comparison:\n",
    "\n",
    "```\n",
    "Linear Regression:           Logistic Regression:\n",
    "       y                            y (probability)\n",
    "       │                            │    1.0 ─────────\n",
    "       │     /                      │         ╱\n",
    "       │    /                       │       ╱\n",
    "       │   /                        │     ╱  (S-curve)\n",
    "       │  /                         │   ╱\n",
    "       │ /                          │ ╱\n",
    "       └─────── x                   └─────────── x\n",
    "                                       0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Load Dataset\n",
    "\n",
    "**Dataset**: Heart Disease UCI Dataset\n",
    "\n",
    "### Dataset Information:\n",
    "- **Source**: UCI Machine Learning Repository\n",
    "- **Target**: Binary classification (0 = No disease, 1 = Disease present)\n",
    "- **Features**: 13 clinical attributes\n",
    "\n",
    "### Features Description:\n",
    "1. **age**: Age in years\n",
    "2. **sex**: Sex (1 = male, 0 = female)\n",
    "3. **cp**: Chest pain type (0-3)\n",
    "4. **trestbps**: Resting blood pressure (mm Hg)\n",
    "5. **chol**: Serum cholesterol (mg/dl)\n",
    "6. **fbs**: Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)\n",
    "7. **restecg**: Resting ECG results (0-2)\n",
    "8. **thalach**: Maximum heart rate achieved\n",
    "9. **exang**: Exercise induced angina (1 = yes, 0 = no)\n",
    "10. **oldpeak**: ST depression induced by exercise\n",
    "11. **slope**: Slope of peak exercise ST segment (0-2)\n",
    "12. **ca**: Number of major vessels colored by fluoroscopy (0-3)\n",
    "13. **thal**: Thalassemia (1 = normal, 2 = fixed defect, 3 = reversible defect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# You can download the dataset from: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
    "# Place it in the dataset folder\n",
    "\n",
    "df = pd.read_csv('../../dataset/heart.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Step 1: Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"📊 Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"📈 Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"🔍 Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"🔍 Duplicate rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"Removing {duplicates} duplicate rows...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"✅ New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print(\"🎯 Target Variable Distribution:\")\n",
    "print(\"=\"*50)\n",
    "target_col = 'target'  # Adjust if your target column has a different name\n",
    "\n",
    "target_counts = df[target_col].value_counts()\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"\\nPercentage:\")\n",
    "print(df[target_col].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x=target_col, ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Heart Disease (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%', \n",
    "            colors=['#90EE90', '#FFB6C1'], startangle=90)\n",
    "axes[1].set_title('Target Variable Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "print(f\"📊 Numerical Features: {len(numerical_cols)}\")\n",
    "print(numerical_cols)\n",
    "\n",
    "# Plot distributions\n",
    "n_cols = 4\n",
    "n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to check for outliers\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(y=df[col], ax=axes[idx], color='lightcoral')\n",
    "    axes[idx].set_title(f'Box Plot of {col}', fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for idx in range(len(numerical_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, linewidths=1, square=True, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix - All Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with target\n",
    "print(\"\\n🎯 Features Most Correlated with Target:\")\n",
    "print(\"=\"*50)\n",
    "target_corr = correlation_matrix[target_col].abs().sort_values(ascending=False)\n",
    "print(target_corr[1:])  # Exclude target itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature comparison by target class\n",
    "features_to_compare = numerical_cols[:4]  # Top 4 features\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_to_compare):\n",
    "    sns.violinplot(data=df, x=target_col, y=feature, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{feature} by Heart Disease Status', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Data Preprocessing\n",
    "\n",
    "### Step 1: Handle Outliers (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using IQR method\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method\n",
    "    \"\"\"\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers_dict[col] = len(outliers)\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "outliers = detect_outliers_iqr(df, numerical_cols)\n",
    "print(\"🔍 Outliers detected per feature:\")\n",
    "print(\"=\"*50)\n",
    "for col, count in outliers.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count} outliers\")\n",
    "\n",
    "# Note: For medical data, we'll keep outliers as they might be medically significant\n",
    "print(\"\\n💡 Note: Keeping outliers as they may represent important medical conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Feature Engineering Examples:\n",
    "# 1. Age groups\n",
    "df_processed['age_group'] = pd.cut(df_processed['age'], \n",
    "                                    bins=[0, 40, 55, 70, 100],\n",
    "                                    labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
    "\n",
    "# 2. Cholesterol categories (if chol exists)\n",
    "if 'chol' in df_processed.columns:\n",
    "    df_processed['chol_category'] = pd.cut(df_processed['chol'],\n",
    "                                           bins=[0, 200, 240, 500],\n",
    "                                           labels=['Desirable', 'Borderline', 'High'])\n",
    "\n",
    "# 3. Blood pressure categories (if trestbps exists)\n",
    "if 'trestbps' in df_processed.columns:\n",
    "    df_processed['bp_category'] = pd.cut(df_processed['trestbps'],\n",
    "                                         bins=[0, 120, 140, 200],\n",
    "                                         labels=['Normal', 'Elevated', 'High'])\n",
    "\n",
    "print(\"✅ Feature engineering completed!\")\n",
    "print(f\"New shape: {df_processed.shape}\")\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_processed.drop([target_col, 'age_group', 'chol_category', 'bp_category'], axis=1, errors='ignore')\n",
    "y = df_processed[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"✅ Data split completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTesting target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better readability\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"✅ Feature scaling completed!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nScaled training data (first 5 rows):\")\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Model Training\n",
    "\n",
    "### Model 1: Logistic Regression (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train basic Logistic Regression model\n",
    "print(\"🤖 Training Logistic Regression Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize and train the model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = log_reg.predict(X_train_scaled)\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_pred_proba = log_reg.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"✅ Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Model Evaluation\n",
    "\n",
    "### Step 1: Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "# Display results\n",
    "print(\"📊 LOGISTIC REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'Training':<20} {'Testing':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Accuracy':<20} {train_accuracy:.4f} ({train_accuracy*100:.2f}%)  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"{'Precision':<20} {train_precision:.4f}            {test_precision:.4f}\")\n",
    "print(f\"{'Recall':<20} {train_recall:.4f}            {test_recall:.4f}\")\n",
    "print(f\"{'F1-Score':<20} {train_f1:.4f}            {test_f1:.4f}\")\n",
    "print(f\"{'ROC-AUC Score':<20} {train_auc:.4f}            {test_auc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Metrics:\n",
    "\n",
    "- **Accuracy**: Overall correctness (correct predictions / total predictions)\n",
    "- **Precision**: Of all positive predictions, how many were actually positive?\n",
    "- **Recall (Sensitivity)**: Of all actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of Precision and Recall\n",
    "- **ROC-AUC**: Area under the ROC curve (1.0 = perfect, 0.5 = random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n📋 CLASSIFICATION REPORT - Testing Set:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No Disease', 'Disease']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add counts to the plot\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "plt.text(0.02, 0.95, f'TN: {TN}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', color='darkblue')\n",
    "plt.text(0.98, 0.95, f'FP: {FP}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='top', horizontalalignment='right', color='darkred')\n",
    "plt.text(0.02, 0.05, f'FN: {FN}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='bottom', color='darkred')\n",
    "plt.text(0.98, 0.05, f'TP: {TP}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='bottom', horizontalalignment='right', color='darkgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Confusion Matrix Breakdown:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True Negatives (TN): {TN} - Correctly predicted No Disease\")\n",
    "print(f\"False Positives (FP): {FP} - Incorrectly predicted Disease\")\n",
    "print(f\"False Negatives (FN): {FN} - Incorrectly predicted No Disease\")\n",
    "print(f\"True Positives (TP): {TP} - Correctly predicted Disease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.5)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📈 ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- AUC = 1.0: Perfect classifier\")\n",
    "print(\"- AUC = 0.9-1.0: Excellent\")\n",
    "print(\"- AUC = 0.8-0.9: Good\")\n",
    "print(\"- AUC = 0.7-0.8: Fair\")\n",
    "print(\"- AUC = 0.5: Random guess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_test_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_test_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower left\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Average Precision Score: {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"🎯 FEATURE IMPORTANCE (by Coefficient Magnitude):\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_importance['Coefficient']]\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance - Logistic Regression Coefficients', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"- Positive coefficients: Increase probability of disease\")\n",
    "print(\"- Negative coefficients: Decrease probability of disease\")\n",
    "print(\"- Larger absolute values: Stronger influence on prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Model Variants with Regularization\n",
    "\n",
    "### Model 2: Logistic Regression with L2 Regularization (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Regularization (Ridge)\n",
    "print(\"🤖 Training Logistic Regression with L2 Regularization...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "log_reg_l2 = LogisticRegression(penalty='l2', C=1.0, random_state=42, max_iter=1000)\n",
    "log_reg_l2.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "y_test_pred_proba_l2 = log_reg_l2.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy_l2 = accuracy_score(y_test, y_test_pred_l2)\n",
    "auc_l2 = roc_auc_score(y_test, y_test_pred_proba_l2)\n",
    "f1_l2 = f1_score(y_test, y_test_pred_l2)\n",
    "\n",
    "print(f\"\\n📊 L2 Regularization Results:\")\n",
    "print(f\"Accuracy: {accuracy_l2:.4f} ({accuracy_l2*100:.2f}%)\")\n",
    "print(f\"ROC-AUC: {auc_l2:.4f}\")\n",
    "print(f\"F1-Score: {f1_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Logistic Regression with L1 Regularization (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Regularization (Lasso)\n",
    "print(\"🤖 Training Logistic Regression with L1 Regularization...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "log_reg_l1 = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', random_state=42, max_iter=1000)\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "y_test_pred_proba_l1 = log_reg_l1.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy_l1 = accuracy_score(y_test, y_test_pred_l1)\n",
    "auc_l1 = roc_auc_score(y_test, y_test_pred_proba_l1)\n",
    "f1_l1 = f1_score(y_test, y_test_pred_l1)\n",
    "\n",
    "print(f\"\\n📊 L1 Regularization Results:\")\n",
    "print(f\"Accuracy: {accuracy_l1:.4f} ({accuracy_l1*100:.2f}%)\")\n",
    "print(f\"ROC-AUC: {auc_l1:.4f}\")\n",
    "print(f\"F1-Score: {f1_l1:.4f}\")\n",
    "\n",
    "# Feature selection by L1\n",
    "selected_features = X_train.columns[log_reg_l1.coef_[0] != 0]\n",
    "print(f\"\\n🎯 Features selected by L1: {len(selected_features)}/{len(X_train.columns)}\")\n",
    "print(f\"Selected features: {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Basic Logistic Regression', 'L2 Regularization (Ridge)', 'L1 Regularization (Lasso)'],\n",
    "    'Accuracy': [test_accuracy, accuracy_l2, accuracy_l1],\n",
    "    'Precision': [test_precision, precision_score(y_test, y_test_pred_l2), precision_score(y_test, y_test_pred_l1)],\n",
    "    'Recall': [test_recall, recall_score(y_test, y_test_pred_l2), recall_score(y_test, y_test_pred_l1)],\n",
    "    'F1-Score': [test_f1, f1_l2, f1_l1],\n",
    "    'ROC-AUC': [test_auc, auc_l2, auc_l1]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 MODEL COMPARISON - All Variants\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.bar(x - width, comparison_df.iloc[0, 1:], width, label='Basic', color='skyblue')\n",
    "ax.bar(x, comparison_df.iloc[1, 1:], width, label='L2 (Ridge)', color='lightcoral')\n",
    "ax.bar(x + width, comparison_df.iloc[2, 1:], width, label='L1 (Lasso)', color='lightgreen')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Comparison - All Regularization Variants', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, model_idx in enumerate([0, 1, 2]):\n",
    "    offset = (i - 1) * width\n",
    "    for j, metric in enumerate(metrics):\n",
    "        value = comparison_df.iloc[model_idx, j+1]\n",
    "        ax.text(j + offset, value + 0.02, f'{value:.3f}', \n",
    "               ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "print(\"🔄 Performing 5-Fold Cross-Validation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\n📊 Cross-Validation Results:\")\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"Min accuracy: {cv_scores.min():.4f}\")\n",
    "print(f\"Max accuracy: {cv_scores.max():.4f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), cv_scores, marker='o', linestyle='-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Cross-Validation Accuracy Scores', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎛️ Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for best hyperparameters\n",
    "print(\"🔍 Performing Grid Search for Hyperparameter Tuning...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000),\n",
    "                          param_grid,\n",
    "                          cv=5,\n",
    "                          scoring='accuracy',\n",
    "                          n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n✅ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred_best = best_model.predict(X_test_scaled)\n",
    "y_test_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, y_test_pred_best)\n",
    "best_auc = roc_auc_score(y_test, y_test_pred_proba_best)\n",
    "best_f1 = f1_score(y_test, y_test_pred_best)\n",
    "\n",
    "print(f\"\\n📊 Best Model Performance on Test Set:\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"ROC-AUC: {best_auc:.4f}\")\n",
    "print(f\"F1-Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Model Persistence (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "\n",
    "# Save model\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"✅ Model and scaler saved successfully!\")\n",
    "print(\"Files: logistic_regression_model.pkl, scaler.pkl\")\n",
    "\n",
    "# Example: Load and use model\n",
    "# with open('logistic_regression_model.pkl', 'rb') as f:\n",
    "#     loaded_model = pickle.load(f)\n",
    "# predictions = loaded_model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Summary and Key Takeaways\n",
    "\n",
    "### 🎯 Model Performance:\n",
    "- Basic Logistic Regression achieved {test_accuracy*100:.2f}% accuracy\n",
    "- ROC-AUC score indicates good discriminative ability\n",
    "- Model successfully identifies heart disease risk\n",
    "\n",
    "### 💡 Key Learnings:\n",
    "\n",
    "1. **Logistic Regression is ideal for binary classification**\n",
    "   - Outputs probabilities between 0 and 1\n",
    "   - Uses sigmoid function to transform linear outputs\n",
    "   - Interpretable coefficients\n",
    "\n",
    "2. **Regularization helps prevent overfitting**\n",
    "   - L2 (Ridge): Shrinks coefficients\n",
    "   - L1 (Lasso): Feature selection\n",
    "   - Choose based on your needs\n",
    "\n",
    "3. **Multiple evaluation metrics are important**\n",
    "   - Accuracy alone isn't enough\n",
    "   - Consider Precision, Recall, F1-Score\n",
    "   - ROC-AUC for overall performance\n",
    "\n",
    "4. **Cross-validation provides reliable estimates**\n",
    "   - Reduces variance in performance estimates\n",
    "   - Helps detect overfitting\n",
    "   - Essential for model selection\n",
    "\n",
    "### 🔄 Linear vs Logistic Regression - Final Comparison:\n",
    "\n",
    "| When to Use | Linear Regression | Logistic Regression |\n",
    "|-------------|------------------|--------------------|\n",
    "| **Task Type** | Regression (predict numbers) | Classification (predict categories) |\n",
    "| **Examples** | House prices, temperature, sales | Spam detection, disease diagnosis, churn |\n",
    "| **Output** | Continuous value | Probability (0-1) |\n",
    "| **Can handle** | Predicting any numeric value | Predicting binary outcomes |\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. Try other classification algorithms (Random Forest, SVM, Neural Networks)\n",
    "2. Implement multi-class classification\n",
    "3. Deploy the model as a web application\n",
    "4. Explore ensemble methods\n",
    "5. Add more features or engineer new ones\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Additional Resources:\n",
    "\n",
    "- [Scikit-Learn Logistic Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n",
    "- [Understanding ROC Curves](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "---\n",
    "\n",
    "**✅ Project Complete! You've successfully implemented Logistic Regression for Heart Disease Prediction!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
