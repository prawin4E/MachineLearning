{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) Implementation\n",
    "\n",
    "This notebook demonstrates a complete implementation of a Recurrent Neural Network for sequence prediction.\n",
    "\n",
    "## Dataset\n",
    "We'll use IMDB movie reviews dataset for sentiment analysis.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Sequential Data Processing\n",
    "2. RNN Architecture\n",
    "3. Text Tokenization and Padding\n",
    "4. Sentiment Analysis\n",
    "5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "vocab_size = 10000\n",
    "max_length = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(f\"Training sequences: {len(X_train)}\")\n",
    "print(f\"Test sequences: {len(X_test)}\")\n",
    "print(f\"\\nSample review (encoded): {X_train[0][:20]}...\")\n",
    "print(f\"Label: {y_train[0]} (1=positive, 0=negative)\")\n",
    "print(f\"\\nReview lengths (first 10): {[len(x) for x in X_train[:10]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.bar(['Negative', 'Positive'], counts)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Sequence length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "seq_lengths = [len(x) for x in X_train]\n",
    "plt.hist(seq_lengths, bins=50, edgecolor='black')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Review Length Distribution')\n",
    "plt.axvline(max_length, color='red', linestyle='--', label=f'Max Length: {max_length}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing - Padding Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to same length\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"\\nSample padded sequence:\\n{X_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RNN\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    \n",
    "    # RNN layers\n",
    "    SimpleRNN(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    SimpleRNN(64),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Compiled Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation metrics\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Decode and Display Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word index\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review if i >= 3])\n",
    "\n",
    "# Show some predictions\n",
    "print(\"Sample Predictions:\\n\")\n",
    "for i in range(5):\n",
    "    review = decode_review(X_test[i])\n",
    "    actual = 'Positive' if y_test[i] == 1 else 'Negative'\n",
    "    predicted = 'Positive' if y_pred[i] == 1 else 'Negative'\n",
    "    confidence = y_pred_prob[i][0]\n",
    "    \n",
    "    print(f\"Review {i+1}:\")\n",
    "    print(f\"Text: {review[:200]}...\")\n",
    "    print(f\"Actual: {actual}, Predicted: {predicted}, Confidence: {confidence:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('rnn_sentiment_model.h5')\n",
    "print(\"Model saved as 'rnn_sentiment_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **RNN Architecture**: Processes sequences with temporal dependencies\n",
    "2. **Embedding Layer**: Converts words to dense vectors\n",
    "3. **Sequential Processing**: Maintains hidden state across time steps\n",
    "4. **Sentiment Analysis**: Binary classification of text\n",
    "5. **Padding**: Uniform sequence lengths for batch processing\n",
    "\n",
    "### When to Use RNN:\n",
    "- Sentiment analysis\n",
    "- Text classification\n",
    "- Time series prediction\n",
    "- Speech recognition\n",
    "- Machine translation (basic)\n",
    "\n",
    "### Advantages:\n",
    "- Handles variable-length sequences\n",
    "- Maintains temporal information\n",
    "- Shares parameters across time steps\n",
    "- Good for short sequences\n",
    "\n",
    "### Limitations:\n",
    "- Vanishing gradient problem\n",
    "- Difficulty learning long-term dependencies\n",
    "- Slower training than feedforward networks\n",
    "- LSTM/GRU often perform better\n",
    "\n",
    "### RNN vs LSTM:\n",
    "- **RNN**: Simpler, faster, but struggles with long sequences\n",
    "- **LSTM**: More complex, better for long-term dependencies\n",
    "- Use RNN for simple/short sequences\n",
    "- Use LSTM for complex/long sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
