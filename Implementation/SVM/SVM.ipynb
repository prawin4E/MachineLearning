{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Support Vector Machine (SVM) Classification\n",
        "\n",
        "## 📊 Project Overview\n",
        "\n",
        "This notebook demonstrates **Support Vector Machine (SVM)** for binary classification - predicting whether a patient has heart disease or not.\n",
        "\n",
        "### 🎯 Learning Objectives:\n",
        "- Understand Support Vector Machine for classification\n",
        "- Compare with Linear Regression and Logistic Regression\n",
        "- Implement complete ML pipeline with SVM\n",
        "- Evaluate classification models with different kernels\n",
        "- Feature importance analysis and hyperparameter tuning\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Linear Regression vs Logistic Regression vs SVM\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect | Linear Regression | Logistic Regression | Support Vector Machine |\n",
        "|--------|------------------|---------------------|------------------------|\n",
        "| **Purpose** | Predict continuous values | Predict probabilities/classes | Find optimal decision boundary |\n",
        "| **Output** | Real number (-∞ to +∞) | Probability (0 to 1) | Class prediction with margin |\n",
        "| **Use Case** | Car prices, house prices | Disease prediction, spam detection | Complex classification, high-dimensional data |\n",
        "| **Function** | y = mx + b | y = 1/(1 + e^-(mx+b)) | f(x) = sign(w·x + b) |\n",
        "| **Loss Function** | Mean Squared Error (MSE) | Log Loss (Cross-Entropy) | Hinge Loss |\n",
        "| **Evaluation** | R², RMSE, MAE | Accuracy, Precision, Recall, F1, ROC-AUC | Accuracy, Precision, Recall, F1, ROC-AUC |\n",
        "| **Algorithm** | Ordinary Least Squares | Maximum Likelihood Estimation | Quadratic Programming |\n",
        "| **Decision Boundary** | Not applicable | Linear boundary | Linear/Non-linear (with kernels) |\n",
        "| **Key Feature** | Best fit line | Sigmoid transformation | Maximum margin separation |\n",
        "| **Handles Non-linearity** | No | No | Yes (with kernel trick) |\n",
        "\n",
        "### When to Use Which?\n",
        "\n",
        "**Use Linear Regression when:**\n",
        "- ✅ Predicting continuous numerical values\n",
        "- ✅ Output can be any real number\n",
        "- ✅ Examples: prices, temperatures, sales\n",
        "\n",
        "**Use Logistic Regression when:**\n",
        "- ✅ Simple binary classification\n",
        "- ✅ Need probability estimates\n",
        "- ✅ Interpretable coefficients required\n",
        "- ✅ Examples: spam/not spam, disease/healthy\n",
        "\n",
        "**Use SVM when:**\n",
        "- ✅ Complex classification problems\n",
        "- ✅ High-dimensional data\n",
        "- ✅ Non-linear relationships (with kernels)\n",
        "- ✅ Small to medium datasets\n",
        "- ✅ Need robust decision boundaries\n",
        "- ✅ Examples: text classification, image recognition, gene classification\n",
        "\n",
        "### Visual Comparison:\n",
        "\n",
        "```\n",
        "Linear Regression:           Logistic Regression:         SVM:\n",
        "       y                            y (probability)           y (class)\n",
        "       │                            │    1.0 ─────────        │    +1 ────────\n",
        "       │     /                      │         ╱               │      ╱│╲\n",
        "       │    /                       │       ╱                 │    ╱  │  ╲\n",
        "       │   /                        │     ╱  (S-curve)        │  ╱    │    ╲\n",
        "       │  /                         │   ╱                     │╱   margin  ╲│\n",
        "       │ /                          │ ╱                       │      │      │\n",
        "       └─────── x                   └─────────── x            └──────│──────── x\n",
        "                                       0.0                           -1\n",
        "                                                              (Maximum Margin)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning - Preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Machine Learning - Models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression  # For comparison\n",
        "\n",
        "# Machine Learning - Evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, roc_auc_score,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📁 Load Dataset\n",
        "\n",
        "**Dataset**: Heart Disease UCI Dataset\n",
        "\n",
        "### Dataset Information:\n",
        "- **Source**: UCI Machine Learning Repository\n",
        "- **Target**: Binary classification (0 = No disease, 1 = Disease present)\n",
        "- **Features**: 13 clinical attributes\n",
        "\n",
        "### Features Description:\n",
        "1. **age**: Age in years\n",
        "2. **sex**: Sex (1 = male, 0 = female)\n",
        "3. **cp**: Chest pain type (0-3)\n",
        "4. **trestbps**: Resting blood pressure (mm Hg)\n",
        "5. **chol**: Serum cholesterol (mg/dl)\n",
        "6. **fbs**: Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)\n",
        "7. **restecg**: Resting ECG results (0-2)\n",
        "8. **thalach**: Maximum heart rate achieved\n",
        "9. **exang**: Exercise induced angina (1 = yes, 0 = no)\n",
        "10. **oldpeak**: ST depression induced by exercise\n",
        "11. **slope**: Slope of peak exercise ST segment (0-2)\n",
        "12. **ca**: Number of major vessels colored by fluoroscopy (0-3)\n",
        "13. **thal**: Thalassemia (1 = normal, 2 = fixed defect, 3 = reversible defect)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# You can download the dataset from: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n",
        "# Place it in the dataset folder\n",
        "\n",
        "df = pd.read_csv('../../dataset/heart.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of samples: {df.shape[0]}\")\n",
        "print(f\"Number of features: {df.shape[1]}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"First 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔍 Exploratory Data Analysis (EDA)\n",
        "\n",
        "### Step 1: Basic Information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset info\n",
        "print(\"📊 Dataset Information:\")\n",
        "print(\"=\"*50)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"📈 Statistical Summary:\")\n",
        "print(\"=\"*50)\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"🔍 Missing Values:\")\n",
        "print(\"=\"*50)\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values found! ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"🔍 Duplicate rows: {duplicates}\")\n",
        "if duplicates > 0:\n",
        "    print(f\"Removing {duplicates} duplicate rows...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"✅ New shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Target Variable Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze target variable distribution\n",
        "print(\"🎯 Target Variable Distribution:\")\n",
        "print(\"=\"*50)\n",
        "target_col = 'target'  # Adjust if your target column has a different name\n",
        "\n",
        "target_counts = df[target_col].value_counts()\n",
        "print(f\"\\nClass Distribution:\")\n",
        "print(target_counts)\n",
        "print(f\"\\nPercentage:\")\n",
        "print(df[target_col].value_counts(normalize=True) * 100)\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Count plot\n",
        "sns.countplot(data=df, x=target_col, ax=axes[0], palette='Set2')\n",
        "axes[0].set_title('Target Variable Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Heart Disease (0=No, 1=Yes)', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "\n",
        "# Pie chart\n",
        "axes[1].pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%', \n",
        "            colors=['#90EE90', '#FFB6C1'], startangle=90)\n",
        "axes[1].set_title('Target Variable Proportion', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔧 Data Preprocessing\n",
        "\n",
        "### Step 1: Prepare Data for Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop(target_col, axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeatures: {list(X.columns)}\")\n",
        "\n",
        "# Split data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"\\n✅ Data split completed!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTesting target distribution:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Scaling (CRITICAL for SVM!)\n",
        "print(\"⚠️  IMPORTANT: SVM requires feature scaling!\")\n",
        "print(\"=\"*50)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for better readability\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"✅ Feature scaling completed!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nScaled training data (first 5 rows):\")\n",
        "X_train_scaled.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🤖 SVM Model Training\n",
        "\n",
        "### Understanding SVM Kernels\n",
        "\n",
        "**SVM Kernel Functions:**\n",
        "- **Linear**: For linearly separable data\n",
        "- **RBF (Radial Basis Function)**: Most popular, handles non-linear patterns\n",
        "- **Polynomial**: For polynomial relationships\n",
        "- **Sigmoid**: Similar to neural networks\n",
        "\n",
        "### Model 1: Linear SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Linear SVM model\n",
        "print(\"🤖 Training Linear SVM Model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize and train the model\n",
        "svm_linear = SVC(kernel='linear', random_state=42, probability=True)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred_linear = svm_linear.predict(X_train_scaled)\n",
        "y_test_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "\n",
        "# Predict probabilities\n",
        "y_train_pred_proba_linear = svm_linear.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_pred_proba_linear = svm_linear.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"✅ Linear SVM training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model 2: RBF SVM (Non-linear)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train RBF SVM model\n",
        "print(\"🤖 Training RBF SVM Model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize and train the model\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42, probability=True)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred_rbf = svm_rbf.predict(X_train_scaled)\n",
        "y_test_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Predict probabilities\n",
        "y_train_pred_proba_rbf = svm_rbf.predict_proba(X_train_scaled)[:, 1]\n",
        "y_test_pred_proba_rbf = svm_rbf.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"✅ RBF SVM training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Model Evaluation\n",
        "\n",
        "### Step 1: Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for both models\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba, model_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'ROC-AUC': auc\n",
        "    }\n",
        "\n",
        "# Calculate metrics for all models\n",
        "linear_metrics = calculate_metrics(y_test, y_test_pred_linear, y_test_pred_proba_linear, 'Linear SVM')\n",
        "rbf_metrics = calculate_metrics(y_test, y_test_pred_rbf, y_test_pred_proba_rbf, 'RBF SVM')\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame([linear_metrics, rbf_metrics])\n",
        "\n",
        "print(\"📊 SVM MODEL COMPARISON - Performance Metrics\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Confusion Matrix Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrices for both models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Linear SVM Confusion Matrix\n",
        "cm_linear = confusion_matrix(y_test, y_test_pred_linear)\n",
        "sns.heatmap(cm_linear, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['No Disease', 'Disease'],\n",
        "            yticklabels=['No Disease', 'Disease'], ax=axes[0])\n",
        "axes[0].set_title('Confusion Matrix - Linear SVM', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Actual Label', fontsize=12)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "# RBF SVM Confusion Matrix\n",
        "cm_rbf = confusion_matrix(y_test, y_test_pred_rbf)\n",
        "sns.heatmap(cm_rbf, annot=True, fmt='d', cmap='Greens', cbar=True,\n",
        "            xticklabels=['No Disease', 'Disease'],\n",
        "            yticklabels=['No Disease', 'Disease'], ax=axes[1])\n",
        "axes[1].set_title('Confusion Matrix - RBF SVM', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Actual Label', fontsize=12)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print confusion matrix breakdown for best model\n",
        "best_model_name = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']\n",
        "if best_model_name == 'Linear SVM':\n",
        "    cm_best = cm_linear\n",
        "else:\n",
        "    cm_best = cm_rbf\n",
        "\n",
        "TN, FP, FN, TP = cm_best.ravel()\n",
        "print(f\"\\n📊 Best Model ({best_model_name}) - Confusion Matrix Breakdown:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"True Negatives (TN): {TN} - Correctly predicted No Disease\")\n",
        "print(f\"False Positives (FP): {FP} - Incorrectly predicted Disease\")\n",
        "print(f\"False Negatives (FN): {FN} - Incorrectly predicted No Disease\")\n",
        "print(f\"True Positives (TP): {TP} - Correctly predicted Disease\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🎛️ Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid Search for best hyperparameters\n",
        "print(\"🔍 Performing Grid Search for SVM Hyperparameter Tuning...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Note: gamma is only used for RBF kernel, but GridSearchCV will handle this\n",
        "grid_search = GridSearchCV(SVC(random_state=42, probability=True),\n",
        "                          param_grid,\n",
        "                          cv=5,\n",
        "                          scoring='accuracy',\n",
        "                          n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\n✅ Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_test_pred_best = best_svm.predict(X_test_scaled)\n",
        "y_test_pred_proba_best = best_svm.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "best_accuracy = accuracy_score(y_test, y_test_pred_best)\n",
        "best_auc = roc_auc_score(y_test, y_test_pred_proba_best)\n",
        "best_f1 = f1_score(y_test, y_test_pred_best)\n",
        "\n",
        "print(f\"\\n📊 Best SVM Model Performance on Test Set:\")\n",
        "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"ROC-AUC: {best_auc:.4f}\")\n",
        "print(f\"F1-Score: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🆚 Comparison with Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression for comparison\n",
        "print(\"🔄 Training Logistic Regression for Comparison...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_test_pred_lr = log_reg.predict(X_test_scaled)\n",
        "y_test_pred_proba_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate metrics for Logistic Regression\n",
        "lr_metrics = calculate_metrics(y_test, y_test_pred_lr, y_test_pred_proba_lr, 'Logistic Regression')\n",
        "\n",
        "# Add to comparison\n",
        "final_comparison = pd.DataFrame([linear_metrics, rbf_metrics, lr_metrics])\n",
        "\n",
        "print(\"\\n📊 FINAL MODEL COMPARISON - SVM vs Logistic Regression\")\n",
        "print(\"=\"*80)\n",
        "print(final_comparison.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Visual comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "ax.bar(x - width, final_comparison.iloc[0, 1:], width, label='Linear SVM', color='skyblue')\n",
        "ax.bar(x, final_comparison.iloc[1, 1:], width, label='RBF SVM', color='lightcoral')\n",
        "ax.bar(x + width, final_comparison.iloc[2, 1:], width, label='Logistic Regression', color='lightgreen')\n",
        "\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Model Comparison - SVM vs Logistic Regression', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "ax.set_ylim([0, 1.1])\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, model_idx in enumerate([0, 1, 2]):\n",
        "    offset = (i - 1) * width\n",
        "    for j, metric in enumerate(metrics):\n",
        "        value = final_comparison.iloc[model_idx, j+1]\n",
        "        ax.text(j + offset, value + 0.02, f'{value:.3f}', \n",
        "               ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📝 Summary and Key Takeaways\n",
        "\n",
        "### 🎯 Model Performance Summary:\n",
        "- **Linear SVM**: Good performance for linearly separable data\n",
        "- **RBF SVM**: Excellent for non-linear patterns with kernel trick\n",
        "- **Best Model**: {best_model_name} achieved highest accuracy\n",
        "\n",
        "### 💡 Key Learnings:\n",
        "\n",
        "1. **SVM is powerful for classification**\n",
        "   - Finds optimal decision boundary with maximum margin\n",
        "   - Handles non-linear data with kernel trick\n",
        "   - Robust to outliers\n",
        "\n",
        "2. **Kernel selection is crucial**\n",
        "   - Linear: Fast, good for linearly separable data\n",
        "   - RBF: Flexible, handles complex patterns\n",
        "   - Polynomial: Good for polynomial relationships\n",
        "\n",
        "3. **Feature scaling is mandatory for SVM**\n",
        "   - SVM is sensitive to feature scales\n",
        "   - Always use StandardScaler or MinMaxScaler\n",
        "   - Critical for proper distance calculations\n",
        "\n",
        "4. **Hyperparameter tuning improves performance**\n",
        "   - C parameter controls regularization\n",
        "   - Gamma parameter affects RBF kernel width\n",
        "   - Use GridSearchCV for systematic optimization\n",
        "\n",
        "### 🔄 Linear vs Logistic vs SVM - Final Comparison:\n",
        "\n",
        "| When to Use | Linear Regression | Logistic Regression | SVM |\n",
        "|-------------|------------------|---------------------|-----|\n",
        "| **Task Type** | Regression (predict numbers) | Simple binary classification | Complex classification |\n",
        "| **Data Size** | Any size | Small to large | Small to medium |\n",
        "| **Non-linearity** | No | No | Yes (with kernels) |\n",
        "| **Interpretability** | High | High | Medium |\n",
        "| **Training Speed** | Fast | Fast | Slower |\n",
        "| **Examples** | House prices, sales | Spam detection, simple diagnosis | Text classification, image recognition |\n",
        "\n",
        "### 🚀 Next Steps:\n",
        "1. Try other SVM kernels (polynomial, sigmoid)\n",
        "2. Implement multi-class SVM classification\n",
        "3. Explore ensemble methods combining SVM with other algorithms\n",
        "4. Deploy the model as a web application\n",
        "5. Add feature selection techniques\n",
        "6. Try other advanced algorithms (Random Forest, XGBoost, Neural Networks)\n",
        "\n",
        "---\n",
        "\n",
        "## 📚 Additional Resources:\n",
        "\n",
        "- [Scikit-Learn SVM Documentation](https://scikit-learn.org/stable/modules/svm.html)\n",
        "- [StatQuest: Support Vector Machines](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
        "- [Understanding SVM Kernels](https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d)\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Project Complete! You've successfully implemented Support Vector Machine for Heart Disease Prediction!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
