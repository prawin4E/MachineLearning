# ğŸš— Car Price Prediction using Linear Regression

A comprehensive machine learning project demonstrating various linear regression techniques for predicting used car prices. This project covers complete data preprocessing, feature engineering, multiple regression models, and detailed performance analysis.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.0%2B-orange)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Features](#-features)
- [Dataset](#-dataset)
- [Installation](#-installation)
- [Usage](#-usage)
- [Model Types](#-model-types)
- [Project Structure](#-project-structure)
- [Results](#-results)
- [Learning Resources](#-learning-resources)
  - [YouTube Tutorials](#youtube-tutorials)
  - [Documentation & Articles](#documentation--articles)
- [Key Learnings](#-key-learnings)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Project Overview

This project implements a complete machine learning pipeline for predicting used car prices using various linear regression techniques. It demonstrates industry-standard practices in:

- **Data Preprocessing**: Handling missing values, duplicates, and outliers
- **Feature Engineering**: Creating meaningful features from raw data
- **Exploratory Data Analysis**: Comprehensive visualization and statistical analysis
- **Model Training**: Implementing 4 types of linear regression
- **Model Evaluation**: Comparing performance using multiple metrics
- **Best Practices**: Following ML workflow standards

---

## âœ¨ Features

### ğŸ”§ Comprehensive Preprocessing
- âœ… Missing value imputation (median/mode strategies)
- âœ… Duplicate detection and removal
- âœ… Outlier detection using IQR method
- âœ… Outlier treatment via Winsorization
- âœ… Feature scaling using StandardScaler
- âœ… Train-test splitting (80-20)

### ğŸ› ï¸ Feature Engineering
- ğŸ·ï¸ Brand extraction from car names
- ğŸ“… Car age calculation
- ğŸ“Š KM per year computation
- ğŸ¯ Categorical binning for analysis
- ğŸ”¢ Label encoding for ordinal features
- ğŸ­ One-hot encoding for nominal features

### ğŸ“Š Visualization & EDA
- Distribution plots (histograms, box plots)
- Correlation heatmaps
- Q-Q plots for normality testing
- Scatter plots for relationships
- Residual analysis
- Feature importance visualization

### ğŸ¤– Multiple Regression Models
1. **Linear Regression (OLS)** - Vanilla implementation
2. **Ridge Regression (L2)** - Handles multicollinearity
3. **Lasso Regression (L1)** - Automatic feature selection
4. **ElasticNet (L1+L2)** - Combined regularization

### ğŸ“ˆ Performance Metrics
- RÂ² Score (Coefficient of Determination)
- RMSE (Root Mean Squared Error)
- MAE (Mean Absolute Error)
- Cross-validation support

---

## ğŸ“ Dataset

**Dataset**: CAR DETAILS FROM CAR DEKHO

### Features:
- **name**: Car model name
- **year**: Manufacturing year
- **selling_price**: Price in INR (Target Variable)
- **km_driven**: Total kilometers driven
- **fuel**: Fuel type (Petrol/Diesel/CNG/LPG)
- **seller_type**: Individual/Dealer/Trustmark Dealer
- **transmission**: Manual/Automatic
- **owner**: First/Second/Third/Fourth & Above

### Dataset Statistics:
- **Samples**: ~4,340 records
- **Features**: 8 original features
- **After Engineering**: 100+ features (post encoding)

---

## ğŸš€ Installation

### Prerequisites
```bash
Python 3.8 or higher
pip (Python package manager)
Jupyter Notebook or JupyterLab
```

### Step 1: Clone the Repository
```bash
git clone <your-repository-url>
cd LinearRegression
```

### Step 2: Create Virtual Environment (Recommended)
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

### Step 3: Install Required Packages
```bash
pip install numpy pandas matplotlib seaborn scikit-learn scipy jupyter
```

Or use requirements.txt:
```bash
pip install -r requirements.txt
```

### Requirements.txt Content:
```txt
numpy>=1.21.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
scipy>=1.7.0
jupyter>=1.0.0
```

---

## ğŸ’» Usage

### Running the Notebook

1. **Start Jupyter Notebook**
   ```bash
   jupyter notebook
   ```

2. **Open the notebook**
   - Navigate to `Implementation/LinearRegression.ipynb`
   - Run cells sequentially from top to bottom

3. **Execute all cells**
   ```
   Cell â†’ Run All
   ```

### Quick Start Code

```python
# Import libraries
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv('dataset/CAR DETAILS FROM CAR DEKHO.csv')

# Preprocess (see notebook for complete pipeline)
# ... preprocessing steps ...

# Train model
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Predict
predictions = model.predict(X_test_scaled)
```

---

## ğŸ“ Model Types

### 1ï¸âƒ£ Linear Regression (OLS)

**Ordinary Least Squares - The Vanilla Approach**

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
```

**When to Use:**
- âœ… Small to medium datasets
- âœ… Low multicollinearity
- âœ… Need interpretable results
- âœ… Baseline model comparison

**Pros:** Simple, fast, interpretable  
**Cons:** Prone to overfitting, sensitive to outliers

---

### 2ï¸âƒ£ Ridge Regression (L2 Regularization)

**Adds L2 penalty: Î± Ã— Î£(Î²Â²)**

```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
```

**When to Use:**
- âœ… High multicollinearity
- âœ… Many correlated features
- âœ… Want to keep all features
- âœ… Prevent overfitting

**Pros:** Handles multicollinearity, reduces overfitting  
**Cons:** Keeps all features (no selection)

---

### 3ï¸âƒ£ Lasso Regression (L1 Regularization)

**Adds L1 penalty: Î± Ã— Î£|Î²|**

```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
```

**When to Use:**
- âœ… Many irrelevant features
- âœ… Need automatic feature selection
- âœ… Want sparse model
- âœ… High-dimensional data

**Pros:** Feature selection, interpretable, sparse models  
**Cons:** Unstable with correlated features

---

### 4ï¸âƒ£ ElasticNet (L1 + L2)

**Combines both penalties: Î± Ã— [l1_ratio Ã— Î£|Î²| + (1-l1_ratio) Ã— Î£(Î²Â²)]**

```python
from sklearn.linear_model import ElasticNet
model = ElasticNet(alpha=1.0, l1_ratio=0.5)
model.fit(X_train, y_train)
```

**When to Use:**
- âœ… Multicollinearity + irrelevant features
- âœ… Best of both worlds
- âœ… Grouped feature selection
- âœ… Complex datasets

**Pros:** Stable, handles both issues, flexible  
**Cons:** Two hyperparameters to tune

---

## ğŸ“‚ Project Structure

```
LinearRegression/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ CAR DETAILS FROM CAR DEKHO.csv    # Raw dataset
â”‚
â”œâ”€â”€ Implementation/
â”‚   â””â”€â”€ LinearRegression.ipynb            # Main Jupyter notebook
â”‚
â”œâ”€â”€ README.md                             # Project documentation
â”œâ”€â”€ requirements.txt                      # Python dependencies
â””â”€â”€ LICENSE                               # License file
```

---

## ğŸ“Š Results

### Model Performance Comparison

| Model | RÂ² Score | RMSE | MAE | Features Used |
|-------|----------|------|-----|---------------|
| Linear Regression | 0.8XXX | â‚¹XX,XXX | â‚¹XX,XXX | All |
| Ridge Regression | 0.8XXX | â‚¹XX,XXX | â‚¹XX,XXX | All |
| Lasso Regression | 0.8XXX | â‚¹XX,XXX | â‚¹XX,XXX | Selected |
| ElasticNet | 0.8XXX | â‚¹XX,XXX | â‚¹XX,XXX | Selected |

*Note: Run the notebook to see actual performance metrics*

### Key Insights

1. **Feature Importance**: Year, brand, and transmission are key predictors
2. **Multicollinearity**: Moderate correlation between features
3. **Outliers**: Significant outliers in price and km_driven
4. **Model Selection**: All models perform similarly (well-structured data)

---

## ğŸ“š Learning Resources

### YouTube Tutorials

#### ğŸ¥ Linear Regression Fundamentals

1. **StatQuest: Linear Regression, Clearly Explained!!!**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=nk2CQITm_eo)
   - ğŸ‘¤ By: StatQuest with Josh Starmer
   - â±ï¸ Duration: 27 minutes
   - ğŸ“ Topics: OLS, RÂ², intuitive explanations

2. **Khan Academy: Introduction to Linear Regression**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=yMgFHbjbAW8)
   - ğŸ‘¤ By: Khan Academy
   - â±ï¸ Duration: 11 minutes
   - ğŸ“ Topics: Basic concepts, least squares

#### ğŸ¥ Regularization Techniques

3. **StatQuest: Regularization Part 1 - Ridge Regression**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=Q81RR3yKn30)
   - ğŸ‘¤ By: StatQuest with Josh Starmer
   - â±ï¸ Duration: 20 minutes
   - ğŸ“ Topics: L2 regularization, bias-variance tradeoff

4. **StatQuest: Regularization Part 2 - Lasso Regression**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=NGf0voTMlcs)
   - ğŸ‘¤ By: StatQuest with Josh Starmer
   - â±ï¸ Duration: 8 minutes
   - ğŸ“ Topics: L1 regularization, feature selection

5. **StatQuest: Ridge vs Lasso Regression, Visualized!!!**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=Xm2C_gTAl8c)
   - ğŸ‘¤ By: StatQuest with Josh Starmer
   - â±ï¸ Duration: 9 minutes
   - ğŸ“ Topics: Comparison, when to use which

6. **ElasticNet Regression Explained**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=1dKRdX9bfIo)
   - ğŸ‘¤ By: Normalized Nerd
   - â±ï¸ Duration: 15 minutes
   - ğŸ“ Topics: Combined regularization, tuning

#### ğŸ¥ Python Implementation

7. **Python Machine Learning: Linear Regression with Scikit-Learn**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=1BYu65vLKdA)
   - ğŸ‘¤ By: Corey Schafer
   - â±ï¸ Duration: 32 minutes
   - ğŸ“ Topics: Scikit-learn, full implementation

8. **Complete Linear Regression Project in Python**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=TJveOYsK6MY)
   - ğŸ‘¤ By: Krish Naik
   - â±ï¸ Duration: 1 hour
   - ğŸ“ Topics: End-to-end project, EDA, deployment

#### ğŸ¥ Feature Engineering & Preprocessing

9. **Feature Engineering for Machine Learning**
   - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=6WDFfaYtN6s)
   - ğŸ‘¤ By: Krish Naik
   - â±ï¸ Duration: 45 minutes
   - ğŸ“ Topics: Creating features, transformations

10. **Data Preprocessing in Machine Learning**
    - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=7PtFLfPyHqI)
    - ğŸ‘¤ By: Simplilearn
    - â±ï¸ Duration: 30 minutes
    - ğŸ“ Topics: Scaling, encoding, handling missing data

#### ğŸ¥ Model Evaluation

11. **Understanding R-Squared, Adjusted R-Squared, and RMSE**
    - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=nRFCjZbSR4w)
    - ğŸ‘¤ By: Data School
    - â±ï¸ Duration: 16 minutes
    - ğŸ“ Topics: Evaluation metrics, interpretation

12. **Cross Validation Explained**
    - ğŸ“º [Watch Video](https://www.youtube.com/watch?v=fSytzGwwBVw)
    - ğŸ‘¤ By: StatQuest with Josh Starmer
    - â±ï¸ Duration: 6 minutes
    - ğŸ“ Topics: K-fold CV, validation strategies

---

### Documentation & Articles

#### ğŸ“– Official Documentation

1. **Scikit-Learn: Linear Models**
   - ğŸ”— [Read Documentation](https://scikit-learn.org/stable/modules/linear_model.html)
   - ğŸ“ Complete guide to all linear models in sklearn

2. **Scikit-Learn: Preprocessing**
   - ğŸ”— [Read Documentation](https://scikit-learn.org/stable/modules/preprocessing.html)
   - ğŸ“ Data preprocessing techniques

3. **Pandas Documentation**
   - ğŸ”— [Read Documentation](https://pandas.pydata.org/docs/)
   - ğŸ“ Data manipulation and analysis

#### ğŸ“ In-Depth Articles

4. **Introduction to Linear Regression**
   - ğŸ”— [Read Article](https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0)
   - ğŸ“° Towards Data Science
   - ğŸ“ Theory + Python implementation

5. **Ridge and Lasso Regression: A Complete Guide**
   - ğŸ”— [Read Article](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b)
   - ğŸ“° Towards Data Science
   - ğŸ“ Detailed comparison with examples

6. **ElasticNet Regression Explained**
   - ğŸ”— [Read Article](https://machinelearningmastery.com/elastic-net-regression-in-python/)
   - ğŸ“° Machine Learning Mastery
   - ğŸ“ Practical guide with code

7. **Feature Engineering Techniques**
   - ğŸ”— [Read Article](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)
   - ğŸ“° Towards Data Science
   - ğŸ“ Best practices and techniques

8. **Understanding the Bias-Variance Tradeoff**
   - ğŸ”— [Read Article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)
   - ğŸ“° Towards Data Science
   - ğŸ“ Why regularization works

#### ğŸ“ Interactive Tutorials

9. **Kaggle: Introduction to Machine Learning**
   - ğŸ”— [Start Course](https://www.kaggle.com/learn/intro-to-machine-learning)
   - ğŸ“ Interactive Python notebooks

10. **Google's Machine Learning Crash Course**
    - ğŸ”— [Start Course](https://developers.google.com/machine-learning/crash-course)
    - ğŸ“ Free course with TensorFlow examples

#### ğŸ“Š Cheat Sheets

11. **Scikit-Learn Cheat Sheet**
    - ğŸ”— [Download PDF](https://www.datacamp.com/cheat-sheet/scikit-learn-cheat-sheet-python-machine-learning)
    - ğŸ“ Quick reference for sklearn functions

12. **Pandas Cheat Sheet**
    - ğŸ”— [Download PDF](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)
    - ğŸ“ Essential pandas operations

---

### ğŸ¯ Recommended Learning Path

```
1. Start: Watch StatQuest Linear Regression video
           â†“
2. Theory: Read Scikit-Learn Linear Models documentation
           â†“
3. Practice: Follow Corey Schafer's implementation tutorial
           â†“
4. Deep Dive: Watch regularization videos (Ridge, Lasso)
           â†“
5. Advanced: Read articles on feature engineering
           â†“
6. Project: Work through this notebook!
           â†“
7. Deploy: Build your own car price predictor
```

---

## ğŸ’¡ Key Learnings

### ğŸ¯ When to Use Which Model?

| Scenario | Recommended Model |
|----------|------------------|
| **Few features, no correlation** | Linear Regression |
| **Many correlated features** | Ridge Regression |
| **Many features, some irrelevant** | Lasso Regression |
| **Both issues: correlation + irrelevant** | ElasticNet |
| **Unsure? Start with** | Linear Regression (baseline) |

### ğŸ”‘ Best Practices

1. **Always preprocess data thoroughly**
   - Handle missing values
   - Detect and treat outliers
   - Scale features
   - Encode categorical variables

2. **Perform EDA before modeling**
   - Understand data distribution
   - Check for correlations
   - Identify patterns and anomalies

3. **Compare multiple models**
   - Start with baseline (Linear Regression)
   - Try regularized versions
   - Use cross-validation
   - Select based on metrics + interpretability

4. **Monitor for overfitting**
   - Compare train vs test performance
   - Use regularization when needed
   - Simplify models when possible

5. **Document everything**
   - Keep track of experiments
   - Document preprocessing steps
   - Record model parameters
   - Save results for comparison

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. **Commit your changes**
   ```bash
   git commit -m 'Add some AmazingFeature'
   ```
4. **Push to the branch**
   ```bash
   git push origin feature/AmazingFeature
   ```
5. **Open a Pull Request**

### Ideas for Contribution
- Add more visualization techniques
- Implement cross-validation
- Add polynomial features
- Try ensemble methods
- Improve feature engineering
- Add model deployment code

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Dataset**: CarDekho for providing the car price dataset
- **Libraries**: Scikit-Learn, Pandas, NumPy, Matplotlib, Seaborn
- **Inspiration**: Various Kaggle notebooks and tutorials
- **Community**: Stack Overflow and Data Science community

---

## ğŸ“ Contact

**Project Maintainer**: Pravin Kumar S

- ğŸ“§ Email: [Your Email]
- ğŸ’¼ LinkedIn: [Your LinkedIn]
- ğŸ™ GitHub: [Your GitHub]

---

## ğŸ”® Future Enhancements

- [ ] Add cross-validation implementation
- [ ] Implement polynomial regression
- [ ] Add ensemble methods (Random Forest, XGBoost)
- [ ] Create Flask/Streamlit web app
- [ ] Add model persistence (pickle/joblib)
- [ ] Implement GridSearchCV for hyperparameter tuning
- [ ] Add more feature engineering techniques
- [ ] Create interactive dashboards
- [ ] Add model interpretability (SHAP values)
- [ ] Deploy to cloud (AWS/Azure/GCP)

---

## ğŸ“š Additional Resources

### Books ğŸ“–
1. **"An Introduction to Statistical Learning"** by Gareth James et al.
   - Free PDF: http://faculty.marshall.usc.edu/gareth-james/ISL/

2. **"Hands-On Machine Learning"** by AurÃ©lien GÃ©ron
   - GitHub: https://github.com/ageron/handson-ml2

### Courses ğŸ“
1. **Andrew Ng's Machine Learning Course**
   - Coursera: https://www.coursera.org/learn/machine-learning

2. **Fast.ai Practical Deep Learning**
   - Website: https://www.fast.ai/

---

<div align="center">

### â­ If you found this project helpful, please give it a star! â­

**Happy Learning! ğŸš€**

---

Made with â¤ï¸ by Pravin Kumar S

</div>
